{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Structural Analysis and Visualization of Networks</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center/>Course Project #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Student: *{Your Name}*</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <hr /> General Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hard Deadline:** 21.06.2015 23:59 <br \\>\n",
    "\n",
    "Please send your reports to <mailto:leonid.e.zhukov@gmail.com> and <mailto:shestakoffandrey@gmail.com> with message subject of the following structure:<br \\> **[HSE Networks 2015] *{LastName}* *{First Name}* Project*{Number}***\n",
    "\n",
    "Support your computations with figures and comments. <br \\>\n",
    "If you are using IPython Notebook you may use this file as a starting point of your report.<br \\>\n",
    "<br \\>\n",
    "<hr \\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are provided with the [DBLP dataset](https://www.dropbox.com/s/ft4ekv2f3r43u7b/dblp_2000.csv.gz?dl=0) (warning, raw data!). It contains coauthorships that were revealed during $2000$-$2014$. Particularly, the file contains $3$ colomns: first two for authors' names and the third for the year of publication. This data can be naturally mapped to undirected graph structure.\n",
    "\n",
    "Your task is construct supervised link prediction scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Use *pandas* module to load and manipulate the dataset in Python\n",
    "1. Initiallize your classification set as follows:\n",
    "    * Determine training and testing intervals on your time domain (for instance, in DBLP dataset take a period $2000$-$2010$ as training period and $2011$-$2014$ as testing period)\n",
    "    * Pick pairs of authors that **have appeared during training interval** but **have not published together** during it\n",
    "    * These pairs form **positive** or **negative** examples depending on whether they have formed coauthorships **during the testing interval**\n",
    "    * You have arrived to binary classification problem. PROFIT!\n",
    "2. Construct feature space:\n",
    "    * Most of our features tend to be topological. Examples of the features can be: (weighted) sum of neigbours, shortest distance, etc\n",
    "3. Choose at least $4$ classification algorithms from [scikit module](http://scikit-learn.org/stable/) (goes with Anaconda) and compare them in terms of Accuracy, Precision, Recall, F-Score (for positive class) and Mean Squared Error. Use k-fold cross-validation and average your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the [flickr dataset](https://www.dropbox.com/s/srsib3hq863drtp/flickr_data.tar.gz?dl=0) (warning, raw data!). <br/>\n",
    "File ''*users.txt*'' provides a table of form *userID*, *enterTimeStamp*, *additionalInfo*... <br/>\n",
    "File \"*contacts.txt*\" consists of pairs of *userID*'s and link establishment timestamp <br/>\n",
    "\n",
    "Recall *scoring functions* for link prediction. Your task is to compare the performance of each scoring function as follows:\n",
    "1. TOP-$n$ accuracy\n",
    "    * Denote the number of links $E_\\text{new}$ appeared during testing period as $n$\n",
    "    * Denote the ranked list of node pairs provided by score $s$ as $\\hat{E}_s$\n",
    "    * Take top-$n$ pairs from $\\hat{E}_s$ and intersect it with $E_\\text{new}$. Performance is measured as the size of resulted set\n",
    "2. ROC and AUC ('star' subtask)\n",
    "\n",
    "Essentially, for this task you also have to follow the guideline points $1$ and $2$ above. The only thing you have to keep in mind is that flickr dataset is growing dataset. Since then, consider nodes that are significantly represented both in training and testing intervals (for instance, have at least $5$ adjacent edges in training and testing intervals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
